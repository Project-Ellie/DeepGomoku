{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Current Status"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Most recent development\n",
    "\n",
    "ValueTraining.ipynb shows how to create ready-made batches of input data from trajectory strings read from a file\n",
    "\n",
    "- Currently, only the two states leading to the terminal are recorded for the initial value kick-off\n",
    "- Initial value kick-off is a focused training on state values that are certain\n",
    "- symmetries are already considered\n",
    "- all states are created together with their rewards (=values!)\n",
    "- The actions are not recorded yet need to do that as sparse categorical labels\n",
    "- A simple stack of layers demonstrates how badges of input data are being processed\n",
    "\n",
    "HeuristicGameData.ipynb shows how the heuristic policy is used to create those trajectories\n",
    "\n",
    "HeuristicTrajectories.ipynb also observes heuristics plus tree search in action!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next Steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Very Next Step\n",
    "\n",
    "- Identify and walk through a good tutorial with Keras, Tensorboard, datasets. Best appears to be on Tensorflow home page"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network Design\n",
    "- Input is (N+2) x (N+2) x 3  1-padded samples with the third channel representing the border\n",
    "- All is based on a Q-Function\n",
    "- Value Function is tf.reduce_max(Q)\n",
    "- Policy Function is softmax(Q)\n",
    "- Q Function is a deep CNN\n",
    "- Initial version is a stack of 11 x 11 x (3, then 64) x N_filters layers ending in a 1x1x64 combiner with tanh activation\n",
    "- Later versions possibly inspired by RESNET"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Devise a training concept\n",
    "\n",
    "- The underlying Algorithm is off-policy A2C with two AC pairs, like in DDPG\n",
    "- A player/actor is a Q/Policy/Value-Function\n",
    "- We always employ two players: A guiding player and a padawan (learning) player\n",
    "- The first guiding player is the heuristic\n",
    "- Loop:\n",
    "    - With the current guiding policy (with or without treesearch), create trajectories\n",
    "    - Then train the padawan player:\n",
    "        - Always kick off with focused training on the near-terminal states\n",
    "        - To be verified: Use treesearch (guided by the current guiding policy) to carry value information deeper into the trajectory via actor loss\n",
    "        - Then massage the information further with unfocused updates, i.e. train like in all the tutorials\n",
    "    - Let the guiding policy compete with the padawan\n",
    "        - continue until the padawan wins decisively - clone the padawan as the next guiding player\n",
    "    - record all trajectories with metadata in BigQuery"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Activities\n",
    "\n",
    "- Understand the actor loss from theory\n",
    "- Understand how tree search enters the game in training:\n",
    "    - Is learning to advise a tree-search the same as learning to find the very next best move?\n",
    "    - May very well be so, but is it, really?\n",
    "- https://github.com/adventuresinML/adventures-in-ml-code/:\n",
    "    - Learn from a2c_tf2_cartpole.py how to compute the actor loss\n",
    "- Learn how to train keras models with datasets\n",
    "- Create datasets from the trajectories\n",
    "- Create a heuristic player for the game server\n",
    "- export models for the game server"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Far Future\n",
    "\n",
    "- deploy the game server\n",
    "- create a flutter (see: https://filiph.github.io/tictactoe/#/play) frontend for the game server\n",
    "- support competitions on the server, involving humans and AI\n",
    "- participate in gomo-cup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}