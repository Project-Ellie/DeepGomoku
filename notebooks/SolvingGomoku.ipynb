{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Appraoch\n",
    "\n",
    "###\n",
    "### 1) Have a heuristic policy that creates somewhat reasonable terminal trajectories\n",
    "\n",
    "We now have a Policy that is indeed able to play reasonably. [./HeuristicPolicy.ipynb]()\n",
    "\n",
    "###\n",
    "### 2) Have a Trainer Class that does the hard work\n",
    "\n",
    "See [domoku/ddpg.py]()\n",
    "The trainer is able to create arbitrary sets of trajectories\n",
    "\n",
    "###\n",
    "### 3) Have a CNN-based Q-Function ready\n",
    "\n",
    "This Q-Function can feed into a softmax to become a policy and into a max_a layer to become a value function.\n",
    "\n",
    "### Training Friend and Fo at the same time?\n",
    "I believe the *other* player should\n",
    "\n",
    "\n",
    "###\n",
    "### 10) Curriculum\n",
    "Create Trajectories.\n",
    "Do an end-game Bellman training: rewind every trajectory until the remainder is a threat sequence.\n",
    "Then train with\n",
    "\n",
    "$$\n",
    "L(s) = (V(s_n) - \\gamma^{N-n})^2\n",
    "$$\n",
    "\n",
    "Essentially, that will deduct the value of the final couple of states directly from the win/loss value of -1, 1\n",
    "\n",
    "Rewind as many steps as can be covered by the minimax tree search. Use the tree search to further pull value certainty up the trajectory, also do policy updates here.\n",
    "\n",
    "After each complete curriculum pass, exchange the former trajectory creating Q-Function with the new one. Basically cloning the Q Function.\n",
    "\n",
    "Train until the new Q-Function consistently beats the old one. Then clone again.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next steps:\n",
    "\n",
    "- Devise a reasonable CNN architecture and experiment with endgame training using our trajectories\n",
    "- Use rotational symmetry to quadruple the number of sample states\n",
    "- re-introduce the border concept\n",
    "- Use 11x11 convolutions with the missing terminal patterns\n",
    "- Get some experience and intuition about the role of tree search\n",
    "- Read DDPG and alpha go papers\n",
    "- Understand the Policy Updates from a mathematical perspective\n",
    "- Generally, how to properly train with TF/Tensorboard, save and load policy params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}