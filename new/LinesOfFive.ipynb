{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, absolute_import, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting lines of 5 with a ConvNet and hand-woven features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hor=np.zeros([5,5], dtype=float)\n",
    "hor[2]=1\n",
    "diag=np.eye(5, dtype=float)\n",
    "filters = np.array([hor, hor.T, diag, diag[::-1]])\n",
    "kernel_init = tf.constant_initializer(np.rollaxis(filters, 0, 3))\n",
    "bias_init = tf.constant_initializer(-4.)\n",
    "\n",
    "## Take particular note of the shape: Channels last\n",
    "np.shape(kernel_init.value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying  the function with some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boards = np.zeros([6, 10, 10])\n",
    "for i in range(5):\n",
    "    boards[0][5][3+i] = 1.\n",
    "    boards[1][3+i][5] = 1.\n",
    "    boards[2][8-i][3+i] = 1.\n",
    "    boards[3][2+i][2+i] = 1.\n",
    "    boards[4][2+i][2+i] = 1.\n",
    "    boards[5][2+i][2+i] = 1.\n",
    "boards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-610d68918cc8>:4: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/wgiersche/env3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-610d68918cc8>:5: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "inp=tf.constant(boards.reshape(-1,10, 10, 1))\n",
    "out = tf.layers.conv2d(kernel_size=5, kernel_initializer=kernel_init, \n",
    "                       filters=4, inputs=inp, padding='same', \n",
    "                       bias_initializer=bias_init, activation='relu')\n",
    "out = tf.layers.max_pooling2d(inputs=out, pool_size=10, strides=1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    res = sess.run(out)\n",
    "res = np.squeeze(np.rollaxis(res, -1, 0))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that every sample (the six columns) has a $1$ at the position that corresponds to the particular pattern that has been recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating labels with the hand-crafted features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *labels* graph maps each sample that contains a line of 5 to a $1$, all others to a $0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_heuristics = tf.placeholder(name=\"inp_heuristics\", shape=[None, 10, 10, 1], dtype=tf.float32)\n",
    "out = tf.layers.conv2d(kernel_size=5, kernel_initializer=kernel_init, \n",
    "                       filters=4, inputs=inp_heuristics, padding='same', \n",
    "                       bias_initializer=bias_init, activation='relu')\n",
    "out = tf.layers.max_pooling2d(inputs=out, pool_size=10, strides=1)\n",
    "labels = tf.squeeze(tf.sign(tf.reduce_sum(out, axis=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = (np.random.uniform(size = [5, 10,10]) < .3).astype(float).reshape(-1,10,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    res = sess.run(labels, feed_dict={inp_heuristics: samples})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 1., 1., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 1., 1.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.rollaxis(samples[3],-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(sess, placeholder, n=100):\n",
    "    samples = (np.random.uniform(size = [n, 10,10]) < .3).astype(float).reshape(-1,10,10,1)\n",
    "    lbls = sess.run(labels, feed_dict={placeholder: samples})\n",
    "    return samples, lbls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining ResNet and Inception Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design considerations\n",
    "In contrast to Deepmind's network, I'm also using 5x5 filters in an inception [Ref] manner. I use blocks of 3 inception layers with skip connections between the blocks. The skip connections are 1x1 2-filter layers, so that each block's output is mapped into a feature map of 2 channels, which then skips the subsequent block. I'm using a single batch-normalization layer at the end of each block because I accept more risk of overfitting in favour of reducing noise. Gomoku is not about image recognition. The risk of overfitting is come by with zillions of synthetically created boards, anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    \"\"\"\n",
    "    After sufficient training, this instance of ResNet takes an array of dimensions 10x10 \n",
    "    and returns 1 if the array contains the pattern you tought it to recognize.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_blocks):\n",
    "        \n",
    "        self.inps = tf.placeholder(name=\"inp_resnet\", shape=[None, 10, 10, 1], dtype=tf.float32)\n",
    "        self.lbls = tf.placeholder(name=\"lbl_resnet\", shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "        out = self.inps\n",
    "\n",
    "        for i in range(n_blocks):\n",
    "            out = self._res_block(out)\n",
    "\n",
    "        out = tf.layers.conv2d(kernel_size=1, filters=1, inputs=out, padding='same', activation='sigmoid')\n",
    "        self.out = tf.reshape(tf.layers.max_pooling2d(inputs=out, pool_size=10, strides=1), [-1, 1])\n",
    "\n",
    "        self.errors = (self.lbls - self.out)**2\n",
    "        self.accuracy=tf.reduce_sum(tf.cast(self.errors < .1, dtype=tf.int64))\n",
    "\n",
    "        self.loss = tf.losses.mean_squared_error(self.out, self.lbls)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "        self.trainer = self.optimizer.minimize(self.loss)\n",
    "    \n",
    "    \n",
    "    def _res_block(self, inp, filters=16, activation='elu'):\n",
    "      \n",
    "        out1_3 = tf.layers.conv2d(kernel_size=3, filters=filters, inputs=inp, padding='same', activation=activation)\n",
    "        out1_5 = tf.layers.conv2d(kernel_size=5, filters=filters, inputs=inp, padding='same', activation=activation)\n",
    "        out1 = tf.concat([out1_3, out1_5], axis=3)\n",
    "        \n",
    "        out2_3 = tf.layers.conv2d(kernel_size=3, filters=filters, inputs=out1, padding='same', activation=activation)\n",
    "        out2_5 = tf.layers.conv2d(kernel_size=5, filters=filters, inputs=out1, padding='same', activation=activation)\n",
    "        out2 = tf.concat([out2_3, out2_5], axis=3)\n",
    "        \n",
    "        out3_3 = tf.layers.conv2d(kernel_size=3, filters=filters, inputs=out2, padding='same', activation=activation)\n",
    "        out3_5 = tf.layers.conv2d(kernel_size=5, filters=filters, inputs=out2, padding='same', activation=activation)\n",
    "        out3 = tf.concat([out3_3, out3_5], axis=3)\n",
    "        \n",
    "        bn = tf.layers.batch_normalization(inputs=out3)\n",
    "\n",
    "        skip = tf.layers.conv2d(kernel_size=1, filters=2, inputs=inp, padding='same', activation=None)\n",
    "        \n",
    "        return tf.concat([skip, bn], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.40279025: \n",
      "Accuracy 20\n",
      "training loss 0.12310071: \n",
      "Accuracy 661\n",
      "training loss 0.091297686: \n",
      "Accuracy 800\n",
      "training loss 0.03960142: \n",
      "Accuracy 903\n",
      "training loss 0.030346967: \n",
      "Accuracy 959\n",
      "training loss 0.014962292: \n",
      "Accuracy 983\n",
      "training loss 0.0040859394: \n",
      "Accuracy 993\n",
      "training loss 0.0024022518: \n",
      "Accuracy 992\n",
      "training loss 0.00020789387: \n",
      "Accuracy 998\n",
      "training loss 0.0021745246: \n",
      "Accuracy 976\n",
      "training loss 0.0026933313: \n",
      "Accuracy 995\n",
      "training loss 0.0006808436: \n",
      "Accuracy 998\n",
      "training loss 0.00790388: \n",
      "Accuracy 991\n",
      "training loss 0.0019695032: \n",
      "Accuracy 1000\n",
      "training loss 5.5540622e-05: \n",
      "Accuracy 998\n",
      "training loss 7.08283e-05: \n",
      "Accuracy 997\n",
      "training loss 0.00023120517: \n",
      "Accuracy 1000\n",
      "training loss 5.6351633e-05: \n",
      "Accuracy 999\n",
      "training loss 0.0011512511: \n",
      "Accuracy 999\n",
      "training loss 6.771554e-08: \n",
      "Accuracy 1000\n",
      "training loss 3.6179044e-05: \n",
      "Accuracy 1000\n",
      "training loss 1.6873728e-05: \n",
      "Accuracy 999\n",
      "training loss 2.0926971e-07: \n",
      "Accuracy 1000\n",
      "training loss 0.00035017464: \n",
      "Accuracy 999\n",
      "training loss 2.914356e-06: \n",
      "Accuracy 1000\n",
      "training loss 1.8593541e-08: \n",
      "Accuracy 999\n",
      "training loss 0.0001729221: \n",
      "Accuracy 1000\n",
      "training loss 1.6388057e-07: \n",
      "Accuracy 1000\n",
      "training loss 0.00026422672: \n",
      "Accuracy 1000\n",
      "training loss 9.2021786e-08: \n",
      "Accuracy 1000\n",
      "training loss 3.2532864e-08: \n",
      "Accuracy 1000\n",
      "training loss 3.5394776e-09: \n",
      "Accuracy 1000\n",
      "training loss 2.8952074e-06: \n",
      "Accuracy 1000\n",
      "training loss 2.453099e-09: \n",
      "Accuracy 1000\n",
      "training loss 0.0003473296: \n",
      "Accuracy 1000\n",
      "training loss 5.9236736e-09: \n",
      "Accuracy 1000\n",
      "training loss 5.5467e-08: \n",
      "Accuracy 1000\n",
      "training loss 3.5869895e-08: \n",
      "Accuracy 1000\n",
      "training loss 2.4378852e-07: \n",
      "Accuracy 1000\n",
      "training loss 2.502081e-07: \n",
      "Accuracy 1000\n",
      "training loss 2.5830598e-08: \n",
      "Accuracy 1000\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(40001):\n",
    "        smp, lbl = create_samples(sess, inp_heuristics, 500)\n",
    "        lbl = lbl.reshape([-1, 1])\n",
    "        l, o, _ = sess.run([resnet.loss, resnet.out, resnet.trainer], feed_dict={resnet.lbls: lbl, resnet.inps: smp})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"training loss %s: \" % l)\n",
    "            \n",
    "            smp, lbl = create_samples(sess, inp_heuristics, 1000)\n",
    "            lbl = lbl.reshape([-1, 1])\n",
    "            acc, pred, err = sess.run([resnet.accuracy, resnet.out, resnet.errors], feed_dict={resnet.inps: smp, resnet.lbls: lbl})\n",
    "            print(\"Accuracy %s\" % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
