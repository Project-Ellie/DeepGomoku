{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The essence of deep Q learning for Gomoku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network that maps a board to an array of QValues, one for each position, is a Q-Function. In our case, the Q value (estimate) of any position is the total value of the board after that position has been occupied. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We play an epsisode beginning from a (smartly chosen) start. That start could be somewhat in the end state of a game - that get's the episodes closer to the only reward - the win or the loss. Wins may go by some large positive number, losses by a large negative number. \n",
    "\n",
    "In an episode, a network may play black and white, and hence learns to win and to lose in a single go. Note that switching the sides creates virtually unrelated scenarios to learn from. Alternatively the opponent might be modelled by a constant heuristic, or black and white learn exchangingly - one at a time.\n",
    "\n",
    "Interesting to see whether there is a difference if we play episodes with two different players.\n",
    "\n",
    "At the end of an episode, the value of the last board is known and back-propagated in time, since it is the q-value of the last move. The value of the second-but-last board is the loss value $-V_0$. White valuations propagate to the previous white valuations, vice versa. It's important to work with a discount factor to make the algorithm reliably prefer fast victories.\n",
    "\n",
    "$$V^b_{t-1} = \\gamma V^b_t$$\n",
    "\n",
    "$$V^w_{t-1} = \\gamma V^w_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The valuations form a training set that is then feed into a training cycle.\n",
    "At some schedule, the same starting point is then used with the newly-trained algorithm. Note that we have a massive overfitting situation to be considered, as long as we don't do a full-fledged training.\n",
    "\n",
    "Note, though, that looking at the visualized QValues on the board allows us to understand stability and quality of the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
