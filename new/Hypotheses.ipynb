{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Game to Learn From"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgomoku import (\n",
    "    GomokuBoard, HeuristicGomokuPolicy, Heuristics, GomokuTools as gt,\n",
    "    data_from_game)\n",
    "stones=gt.string_to_stones('e10g8g5f5f6e7f7f8e8g9h10d9g10f10h8h9i9g7e9j8h11i12e11e12g11f11f12e13g13h14i10g14j9k8i8k4i7')\n",
    "heuristics = Heuristics(kappa=3.0)\n",
    "stones = stones[:-5]\n",
    "print(stones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = GomokuBoard(N=board_size, disp_width=10, heuristics=heuristics, stones=stones)\n",
    "board.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GomokuData import create_sample, to_matrix12\n",
    "s = create_sample(board.stones, 20, 0)\n",
    "to_matrix12(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Deep Q-Function\n",
    "This function is designed to integrate into the estimator training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_gomoku(board_size, features, feature_columns, options):\n",
    "\n",
    "    N = board_size\n",
    "    \n",
    "    layout = options['layout']\n",
    "    \n",
    "    feature_columns = [num('state', shape=((N+2)*(N+2)*2))]\n",
    "\n",
    "    input_layer = tf.feature_column.input_layer( \n",
    "        features, feature_columns=feature_columns)\n",
    "\n",
    "    layer = tf.reshape(input_layer, [-1, N+2, N+2, 2], name='reshape_input') \n",
    "   \n",
    "    for filters, kernel in np.reshape(layout, [-1,2]):\n",
    "        layer = tf.layers.conv2d(inputs=layer, filters=filters, \n",
    "                                 kernel_size=[kernel, kernel], strides=[1,1], \n",
    "                                 padding='SAME')\n",
    "        \n",
    "        # Exotic! Let the network learn efficient activation functions at each layer\n",
    "        beta_l = tf.Variable(-0.5),\n",
    "        beta_r = tf.Variable(0.5)\n",
    "        layer = layer * (layer - beta_l) * (layer - beta_r)\n",
    "        \n",
    "    layer = tf.layers.conv2d(inputs=layer, filters=1, \n",
    "                              kernel_size=[kernel, kernel], strides=[1,1], \n",
    "                             padding='SAME')\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = tf.reshape(tf.constant(s, dtype=tf.float32), [-1, 968])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.feature_column import numeric_column as num\n",
    "features = {'state': [state] * 5}\n",
    "feature_columns = [num('state', shape=((board_size+2)*(board_size+2)*2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout=[128, 3, 128, 3, 128, 3, 64, 3, 64, 3, 16, 3]\n",
    "options={'layout': layout, 'learning_rate': 1e-4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = conv_gomoku(20, features, feature_columns, options)\n",
    "qf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Masking the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones([22, 22], dtype=int)\n",
    "mask[0] = 0\n",
    "mask[21] = 0\n",
    "mask[:,0]=0\n",
    "mask[:,21]=0\n",
    "mask = tf.constant(mask, dtype=tf.float32)\n",
    "mask = tf.expand_dims(mask,-1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qfm = qf * mask\n",
    "qfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    q, m, qm = sess.run([qf, mask, qfm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.shape, m.shape, qm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvalues = np.rollaxis(qm[0], 2, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvalues[:2], qvalues.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Labels From Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgomoku import heuristic_QF, wrap_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = HeuristicGomokuPolicy(bias=0.5, topn=5, style=2)\n",
    "hqf, dval = heuristic_QF(board, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqf = wrap_sample(hqf, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqf[0:2], hqf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqf = hqf/100.0 # helps converge faster\n",
    "hqf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to QValuate a Single Situation\n",
    "This is just to verify that the exotic choices in the hypotheses still provide good convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s5 = [s]*5\n",
    "hqf5 = np.array([hqf]*5)\n",
    "hqf5 = np.expand_dims(hqf5,-1)\n",
    "hqf5.shape, np.shape(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inputs, labels, options):\n",
    "    \"\"\"\n",
    "    samples and labels must be ndarrays of shape (N,22,22,2) and (N,22,22,1) resp.\n",
    "    \"\"\"\n",
    "\n",
    "    learning_rate=options['learning_rate']\n",
    "    \n",
    "    mask = np.ones([22, 22], dtype=int)\n",
    "    mask[0] = 0\n",
    "    mask[21] = 0\n",
    "    mask[:,0]=0\n",
    "    mask[:,21]=0\n",
    "    mask = tf.constant(mask, dtype=tf.float32)\n",
    "    mask = tf.expand_dims(mask,-1)\n",
    "    \n",
    "    inputs = [tf.reshape(tf.constant(sample, dtype=tf.float32), [-1, 968]) \n",
    "              for sample in inputs]\n",
    "    \n",
    "    from tensorflow.feature_column import numeric_column as num\n",
    "    feature_columns = [num('state', shape=((board_size+2)*(board_size+2)*2))]\n",
    "    \n",
    "    #inputs = {'state': [ sample for sample in inputs ]}\n",
    "    features = {'state': inputs}\n",
    "    qf = conv_gomoku(20, features, feature_columns, options)    \n",
    "    \n",
    "    labels = tf.constant(labels, dtype=tf.float32)\n",
    "    loss = tf.losses.mean_squared_error(labels, qf * mask)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return qf * 100.0, optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf, opt, loss = create_model(s5, hqf5 / 100.0, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in range(3001):\n",
    "        _ = session.run(opt)\n",
    "        if i % 200 == 0:\n",
    "            l = session.run(loss)\n",
    "            l_t = session.run(loss)\n",
    "            print(l, l_t)\n",
    "    res=session.run(100.0 * qf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.rollaxis(res[0], 2, 0)[0][8].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the first and last field in this 8th row are effectively beyond the border and don't contribute to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hqf*100.0)[8].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from 8 different  samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgomoku import create_samples_and_qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, qvalues, _ = create_samples_and_qvalues(board, policy, heuristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.shape, qvalues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf, opt, loss = create_model(states, qvalues / 100.0, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in range(5001):\n",
    "        _ = session.run(opt)\n",
    "        if i % 500 == 0:\n",
    "            l = session.run(loss)\n",
    "            l_t = session.run(loss)\n",
    "            print(l, l_t)\n",
    "    res=session.run(qf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All learned Q-Values are within a tolerance of $\\pm 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 10.0\n",
    "for i in range(8):\n",
    "    deepq = np.rollaxis(res[i], 2, 0)[0][1:-1].T[1:-1].T\n",
    "    qvals = np.rollaxis(qvalues[i], 2, 0)[0][1:-1].T[1:-1].T\n",
    "    print ((deepq - qvals > -tolerance).all() & (deepq - qvals < tolerance).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn From the Game's Entire History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from wgomoku import data_from_game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, qvalues = data_from_game(deepcopy(board), policy, heuristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.shape, qvalues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options['learning_rate'] = 1e-4\n",
    "qf, opt, loss = create_model(states, qvalues / 100.0, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in range(2001):\n",
    "        _ = session.run(opt)\n",
    "        if i % 1000 == 0:\n",
    "            l = session.run(loss)\n",
    "            l_t = session.run(loss)\n",
    "            print(l, l_t)\n",
    "    res=session.run(qf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    deepq = np.rollaxis(res[i], 2, 0)[0][1:-1].T[1:-1].T\n",
    "    qvals = np.rollaxis(qvalues[i], 2, 0)[0][1:-1].T[1:-1].T\n",
    "    print(max(np.max(deepq-qvals), -np.min(deepq-qvals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not going to work at all. Let's look at the sixth row of all the 24 different games states. We can see that the network is particularly bad at critical states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, 192, 8):\n",
    "    default_value = qvalues[idx][0][0]\n",
    "    deepq = np.rollaxis(res[idx], 2, 0)[0][1:-1].T[1:-1].T\n",
    "    qvals = np.rollaxis(qvalues[idx], 2, 0)[0][1:-1].T[1:-1].T\n",
    "    print(\"Default: %s\" % default_value)\n",
    "    print(\"Differences: %s\" % (deepq - qvals).astype(int)[6])\n",
    "    print(\"Heuristics: %s\" % qvals.astype(int)[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgomoku import to_matrix_xo\n",
    "def to_matrix_xo(sample):\n",
    "    if np.sum(to_matrix12(sample)>0) % 2 == 0:\n",
    "        symbols = ['. ', 'x ', 'o ']\n",
    "    else:\n",
    "        symbols = ['. ', 'o ', 'x ']\n",
    "    im12 = to_matrix12(sample)\n",
    "    return \"\\n\".join([\"\".join([symbols[c] for c in im12[r]]) for r in range(20) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_matrix_xo(states[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_matrix_xo(states[8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning A-Values Instead\n",
    "The reason is obviously the massive variance of the default QValue. QValues are hard to learn. Let's try to learn Advantage values instead. In our case we can easily identify the default value used in the heuristic QFunction, since the beyond-border fields are filled by that value. Subtracting that default value from all fields provides a pretty good estimate for an advantage function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(192):\n",
    "    avalues[i] = qvalues[i]-qvalues[i][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options['learning_rate'] = 1e-4\n",
    "qf, opt, loss = create_model(states, avalues / 100.0, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in range(10001):\n",
    "        _ = session.run(opt)\n",
    "        if i % 100 == 0:\n",
    "            l = session.run(loss)\n",
    "            l_t = session.run(loss)\n",
    "            print(l, l_t)\n",
    "    res=session.run(qf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, 192, 8):\n",
    "    default_value = qvalues[idx][0][0]\n",
    "    deepq = np.rollaxis(res[idx], 2, 0)[0][1:-1].T[1:-1].T + default_value\n",
    "    qvals = np.rollaxis(qvalues[idx], 2, 0)[0][1:-1].T[1:-1].T\n",
    "    print(\"Default: %s\" % default_value)\n",
    "    print(\"Differences: %s\" % (deepq - qvals).astype(int)[6])\n",
    "    print(\"Heuristics: %s\" % qvals.astype(int)[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=32\n",
    "default_value = qvalues[idx][0][0]\n",
    "deepq = np.rollaxis(res[idx], 2, 0)[0][1:-1].T[1:-1].T + default_value\n",
    "qvals = np.rollaxis(qvalues[idx], 2, 0)[0][1:-1].T[1:-1].T\n",
    "deepq.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Summary\n",
    "The advantage function can obviously be efficiently learned. The Q-Function is too hard to understand for any neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
