{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from wgomoku import (\n",
    "    GomokuBoard, Heuristics, HeuristicGomokuPolicy, GomokuTools as gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U = \\\n",
    "    1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21\n",
    "BLACK=0\n",
    "WHITE=1\n",
    "EDGES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristics = Heuristics(kappa=3.0)\n",
    "board = GomokuBoard(heuristics, N=15, disp_width=8)\n",
    "policy = HeuristicGomokuPolicy(bias=.5, style=2, topn=5)\n",
    "board.set(H,8).set('G',6).set(G,8).set(F,8).set(H,9).set(H,10)\n",
    "board.display('current');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variants_for(board):\n",
    "    \"\"\"\n",
    "    Create a tensor 8x2xNxN to represent the 8 equivalent boards \n",
    "    that can be created from the stones by reflection and rotation.\n",
    "    \"\"\"\n",
    "    stones = board.stones.copy()\n",
    "    N = board.N\n",
    "    array=np.zeros([8,2,N,N], dtype=float)\n",
    "    color = np.arange(len(stones)) % 2\n",
    "    for l, pos in list(zip(color, stones)):\n",
    "        r, c = gt.b2m(pos, 15)\n",
    "        array[0][l][r][c] = 1.0\n",
    "        array[6][l][c][r] = 1.0\n",
    "\n",
    "        array[1][l][c][N-r] = 1.0\n",
    "        array[4][l][N-r][c] = 1.0\n",
    "\n",
    "        array[3][l][N-c][r] = 1.0\n",
    "        array[7][l][r][N-c] = 1.0\n",
    "\n",
    "        array[2][l][N-r][N-c] = 1.0\n",
    "        array[5][l][N-c][N-r] = 1.0\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = variants_for(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at one of the boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation = 3\n",
    "mt[orientation][0] + 2*mt[orientation][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play and record a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_out(heuristics):\n",
    "    board = GomokuBoard(heuristics, N=15, disp_width=8)\n",
    "    policy = HeuristicGomokuPolicy(bias=.5, style=2, topn=5)\n",
    "    board.set(H,8).set('G',6).set(G,8).set(F,8).set(H,9).set(H,10)\n",
    "    n = 0\n",
    "    board.compute_all_scores()\n",
    "    move = policy.suggest(board)\n",
    "    while move.status == 0 and n < 100:\n",
    "        board.set(move.x,move.y)\n",
    "        board.compute_all_scores()\n",
    "        move = policy.suggest(board)\n",
    "        n+=1\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = roll_out(heuristics)\n",
    "board.display('current')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Heuristic Q Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = GomokuBoard(heuristics, N=15, disp_width=8)\n",
    "policy = HeuristicGomokuPolicy(bias=.5, style=2, topn=5)\n",
    "board.set(H,8).set('G',6).set(G,8).set(F,8).set(H,9).set(H,10).set(F,7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.display('current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A meaningful definition for the heuristic Q function\n",
    "\n",
    "Note that this function is just the initial estimate for the Q-function that we'll set out to find with Deep QLearning\n",
    "\n",
    "- If there is at least one serious threat >= 6.9, then every QValue other than those threats must be -9999.\n",
    "\n",
    "- If there is at least one win >= 6.9 then all winners get 9999, every QValue other than those is simply $0$\n",
    "\n",
    "- If there is no critical value, the QValue estimate of any one of the the N best moves is the value of the board after taking that move and observing the best response. For the sake of computational efficiency, all other fields' QValues are approximated by the least significant move followed by the best response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QFunction import enumerated_top\n",
    "enumerated_top(board, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QFunction import heuristic_QF, value_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.display('current')\n",
    "q = heuristic_QF(board, policy)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plausibly, the heuristic Q function reflects the policy's choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The policy\n",
    "move = policy.suggest(board, topn=1, bias=0.02)\n",
    "print(\"Suggested Move: \" + move.__repr__()) \n",
    "print(\"Value after the move: \" + str(value_after(board, (move.x, move.y), policy)))\n",
    "\n",
    "## The Q function\n",
    "r,c=gt.b2m((move.x, move.y), board.N)\n",
    "print(\"Q Value: \" + str(q[0][r][c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
